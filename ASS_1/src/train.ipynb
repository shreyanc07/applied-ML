{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rickc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_data(file_path):\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(train_data, y_train, model_name='logistic_regression'):\n",
    "    if model_name == 'logistic_regression':\n",
    "        model = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "            ('clf', LogisticRegression(random_state=42))\n",
    "        ])\n",
    "    elif model_name == 'naive_bayes':\n",
    "        model = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "            ('clf', MultinomialNB())\n",
    "        ])\n",
    "    elif model_name == 'lightgbm':\n",
    "        model = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "            ('clf', lgb.LGBMClassifier(random_state=42,force_row_wise=True))\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Model name not recognized. Choose 'logistic_regression', 'naive_bayes', or 'lightgbm'\")\n",
    "    \n",
    "    model.fit(train_data, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, data, y_true):\n",
    "    y_pred = model.predict(data)\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, train_data, y_train, validation_data, y_val):\n",
    "    print(\"Train score:\", score_model(model, train_data, y_train))\n",
    "    print(\"Validation score:\", score_model(model, validation_data, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "model_names = ['logistic_regression', 'naive_bayes', 'lightgbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = load_split_data(r'C:\\CMI\\Applied ML\\ASS_1\\data\\dataset\\train.csv')\n",
    "validation_data = load_split_data(r'C:\\CMI\\Applied ML\\ASS_1\\data\\dataset\\validation.csv')\n",
    "test_data = load_split_data(r'C:\\CMI\\Applied ML\\ASS_1\\data\\dataset\\test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['text']\n",
    "y_train = train_data['spam']\n",
    "X_val = validation_data['text']\n",
    "y_val = validation_data['spam']\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with logistic_regression:\n",
      "Train score: 0.9949803579223047\n",
      "Validation score: 0.9825378346915018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models:  33%|███▎      | 1/3 [00:01<00:03,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      3504\n",
      "           1       1.00      0.98      0.99      1078\n",
      "\n",
      "    accuracy                           0.99      4582\n",
      "   macro avg       1.00      0.99      0.99      4582\n",
      "weighted avg       0.99      0.99      0.99      4582\n",
      "\n",
      "For validation.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       645\n",
      "           1       1.00      0.93      0.96       214\n",
      "\n",
      "    accuracy                           0.98       859\n",
      "   macro avg       0.99      0.97      0.98       859\n",
      "weighted avg       0.98      0.98      0.98       859\n",
      "\n",
      "\n",
      "Training with naive_bayes:\n",
      "Train score: 0.9423832387603667\n",
      "Validation score: 0.889406286379511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models:  67%|██████▋   | 2/3 [00:03<00:01,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      3504\n",
      "           1       1.00      0.76      0.86      1078\n",
      "\n",
      "    accuracy                           0.94      4582\n",
      "   macro avg       0.96      0.88      0.91      4582\n",
      "weighted avg       0.95      0.94      0.94      4582\n",
      "\n",
      "For validation.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       645\n",
      "           1       1.00      0.56      0.71       214\n",
      "\n",
      "    accuracy                           0.89       859\n",
      "   macro avg       0.94      0.78      0.82       859\n",
      "weighted avg       0.90      0.89      0.88       859\n",
      "\n",
      "\n",
      "Training with lightgbm:\n",
      "[LightGBM] [Info] Number of positive: 1078, number of negative: 3504\n",
      "[LightGBM] [Info] Total Bins 101384\n",
      "[LightGBM] [Info] Number of data points in the train set: 4582, number of used features: 3092\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.235268 -> initscore=-1.178798\n",
      "[LightGBM] [Info] Start training from score -1.178798\n",
      "Train score: 1.0\n",
      "Validation score: 0.9848661233993015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|██████████| 3/3 [00:06<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3504\n",
      "           1       1.00      1.00      1.00      1078\n",
      "\n",
      "    accuracy                           1.00      4582\n",
      "   macro avg       1.00      1.00      1.00      4582\n",
      "weighted avg       1.00      1.00      1.00      4582\n",
      "\n",
      "For validation.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       645\n",
      "           1       0.96      0.98      0.97       214\n",
      "\n",
      "    accuracy                           0.98       859\n",
      "   macro avg       0.98      0.98      0.98       859\n",
      "weighted avg       0.98      0.98      0.98       859\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation \n",
    "for model_name in tqdm(model_names, desc=\"Training Models\"):\n",
    "    print(f\"\\nTraining with {model_name}:\")\n",
    "    model = fit_model(X_train, y_train, model_name)\n",
    "    \n",
    "    # Score on train and validation\n",
    "    validate_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Evaluate on train and validation\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    print(\"For training.\\n\")\n",
    "    evaluate_model(y_train, y_pred_train)\n",
    "    print(\"For validation.\\n\")\n",
    "    evaluate_model(y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Grid Search CV , LightGBM scores the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now using Grid Search CV to get the best hyper-parameters for finetunig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the training and validation sets for final fine-tuning\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "models_gcv = {\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ('clf', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    'naive_bayes': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ('clf', MultinomialNB())\n",
    "    ]),\n",
    "    'lightgbm': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ('clf', lgb.LGBMClassifier(random_state=42, force_row_wise=True))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Define the parameter grids for each model\n",
    "param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'tfidf__max_df': [0.5, 0.75],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    },\n",
    "    'naive_bayes': {\n",
    "        'tfidf__max_df': [0.5, 0.75],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'clf__alpha': [0.01, 0.1, 1]\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'tfidf__max_df': [0.5, 0.75],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.5],\n",
    "        'clf__num_leaves': [15,31,63],\n",
    "        'clf__max_depth': [6, 8]\n",
    "           \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Searching Models:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for logistic_regression...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Searching Models:  33%|███▎      | 1/3 [00:37<01:15, 37.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression:\n",
      "{'clf__C': 10, 'tfidf__max_df': 0.5, 'tfidf__ngram_range': (1, 1)}\n",
      "Starting grid search for naive_bayes...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Searching Models:  67%|██████▋   | 2/3 [00:47<00:21, 21.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for naive_bayes:\n",
      "{'clf__alpha': 0.01, 'tfidf__max_df': 0.5, 'tfidf__ngram_range': (1, 2)}\n",
      "Starting grid search for lightgbm...\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "[LightGBM] [Info] Number of positive: 1292, number of negative: 4149\n",
      "[LightGBM] [Info] Total Bins 120518\n",
      "[LightGBM] [Info] Number of data points in the train set: 5441, number of used features: 3458\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.237456 -> initscore=-1.166676\n",
      "[LightGBM] [Info] Start training from score -1.166676\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Searching Models: 100%|██████████| 3/3 [05:28<00:00, 109.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for lightgbm:\n",
      "{'clf__learning_rate': 0.5, 'clf__max_depth': 8, 'clf__num_leaves': 15, 'tfidf__max_df': 0.5, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "for model_name in tqdm(models_gcv.keys(), desc=\"Grid Searching Models\"):\n",
    "    print(f\"Starting grid search for {model_name}...\")\n",
    "    grid_search = GridSearchCV(models_gcv[model_name], param_grids[model_name], cv=3, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_val,y_train_val )\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    # Store the best model\n",
    "    best_models[model_name] = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic_regression': Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.5, stop_words='english')),\n",
       "                 ('clf', LogisticRegression(C=10, random_state=42))]),\n",
       " 'naive_bayes': Pipeline(steps=[('tfidf',\n",
       "                  TfidfVectorizer(max_df=0.5, ngram_range=(1, 2),\n",
       "                                  stop_words='english')),\n",
       "                 ('clf', MultinomialNB(alpha=0.01))]),\n",
       " 'lightgbm': Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.5, stop_words='english')),\n",
       "                 ('clf',\n",
       "                  LGBMClassifier(force_row_wise=True, learning_rate=0.5,\n",
       "                                 max_depth=8, num_leaves=15, random_state=42))])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 25.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression Test Accuracy: 0.98954704\n",
      "naive_bayes Test Accuracy: 0.98257840\n",
      "lightgbm Test Accuracy: 0.98606272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the test accuracy of each model\n",
    "test_accuracies = {}\n",
    "\n",
    "for model_name, model in tqdm(best_models.items()):\n",
    "    # Predicting on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculating accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_accuracies[model_name] = accuracy\n",
    "    \n",
    "    print(f\"{model_name} Test Accuracy: {accuracy:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic_Regression gives the best score after Grid Search CV on test data.\n",
    "\n",
    "-- This may change depending on random state and data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the model with the best test accuracy\n",
    "best_model_name = max(test_accuracies, key=test_accuracies.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report on Test Data for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Best Model: 100%|██████████| 2/2 [00:00<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classification Report for logistic_regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4149\n",
      "           1       1.00      1.00      1.00      1292\n",
      "\n",
      "    accuracy                           1.00      5441\n",
      "   macro avg       1.00      1.00      1.00      5441\n",
      "weighted avg       1.00      1.00      1.00      5441\n",
      "\n",
      "\n",
      "Test Classification Report for logistic_regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       211\n",
      "           1       0.99      0.97      0.98        76\n",
      "\n",
      "    accuracy                           0.99       287\n",
      "   macro avg       0.99      0.98      0.99       287\n",
      "weighted avg       0.99      0.99      0.99       287\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming best_model_name is the name of the best model determined from previous steps\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "# Generate predictions and classification reports for both train and test sets\n",
    "datasets = {\n",
    "    'Train': (X_train_val, y_train_val),\n",
    "    'Test': (X_test, y_test)\n",
    "}\n",
    "\n",
    "for phase, (features, labels) in tqdm(datasets.items(), desc=\"Evaluating Best Model\"):\n",
    "    y_pred = best_model.predict(features)\n",
    "    report = classification_report(labels, y_pred)\n",
    "    print(f\"{phase} Classification Report for {best_model_name}:\\n{report}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
